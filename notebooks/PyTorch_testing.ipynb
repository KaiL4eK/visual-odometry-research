{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import quaternion as quat\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return albu.Compose([\n",
    "#         albu.RandomRotate90(),\n",
    "#         albu.Flip(),\n",
    "#         albu.Transpose(),\n",
    "        albu.OneOf([\n",
    "            albu.IAAAdditiveGaussianNoise(),\n",
    "            albu.GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        albu.OneOf([\n",
    "            albu.MotionBlur(p=0.2),\n",
    "            albu.MedianBlur(blur_limit=3, p=0.1),\n",
    "            albu.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=0, p=0.2),\n",
    "#         albu.OneOf([\n",
    "#             albu.OpticalDistortion(p=0.3),\n",
    "#             albu.GridDistortion(p=0.1),\n",
    "#             albu.IAAPiecewiseAffine(p=0.3),\n",
    "#         ], p=0.2),\n",
    "        albu.OneOf([\n",
    "            albu.CLAHE(clip_limit=2),\n",
    "            albu.IAASharpen(),\n",
    "            albu.IAAEmboss(),\n",
    "            albu.RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        albu.HueSaturationValue(p=0.3),\n",
    "    ], p=0.5,\n",
    "    additional_targets={\"image2\" : \"image\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, transform, debug=False):\n",
    "        self.debug = debug\n",
    "        \n",
    "        SEQUENCE_IDX = '00'\n",
    "        SEQUENCE_DIR = os.path.join(DATASET_DIR, 'sequences', SEQUENCE_IDX)\n",
    "        POSES_DIR = os.path.join(DATASET_DIR, 'poses')\n",
    "\n",
    "        POSE_PATH = os.path.join(POSES_DIR, f'{SEQUENCE_IDX}.txt')\n",
    "        TIMES_PATH = os.path.join(SEQUENCE_DIR, 'times.txt')\n",
    "\n",
    "        self.poses = self._load_poses(POSE_PATH)\n",
    "        self.times = self._load_times(TIMES_PATH)\n",
    "        \n",
    "        self.IMAGES_DIR = os.path.join(SEQUENCE_DIR, 'image_2')\n",
    "        self.images = [fname for fname in os.listdir(self.IMAGES_DIR) if fname.endswith('.png')]\n",
    "        \n",
    "        # Sanity check!\n",
    "        for i in range(len(self.poses)):\n",
    "            fname = self._get_image_fname(i)\n",
    "            if fname not in self.images:\n",
    "                Exception(f'File with name {fname} not exists in {IMAGES_DIR}')\n",
    "        # After this check we can use idx to generate fpaths\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def _get_image_fname(self, idx):\n",
    "        return f'{idx:06}.png'\n",
    "        \n",
    "    def _load_times(self, fpath):\n",
    "        times_data = np.fromfile(fpath, sep='\\n')\n",
    "        return times_data\n",
    "    \n",
    "    def _load_poses(self, fpath):\n",
    "        poses_data = np.fromfile(fpath, sep=' ')\n",
    "        poses_data = poses_data.reshape((-1, 3, 4))\n",
    "        # Convert to 4x4 matrices\n",
    "        last_row = np.array([[[0,0,0,1]]])\n",
    "        last_rows = np.repeat(last_row, axis=0, repeats=poses_data.shape[0])\n",
    "        poses_data = np.hstack((poses_data, last_rows))\n",
    "        return poses_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poses)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_idx = idx\n",
    "        n_idx = idx+1\n",
    "        \n",
    "        c_pose = self.poses[c_idx]\n",
    "        n_pose = self.poses[n_idx]\n",
    "\n",
    "        c_img_fpath = os.path.join(\n",
    "            self.IMAGES_DIR, \n",
    "            self._get_image_fname(c_idx)\n",
    "        )\n",
    "        n_img_fpath = os.path.join(\n",
    "            self.IMAGES_DIR, \n",
    "            self._get_image_fname(n_idx)\n",
    "        )\n",
    "        \n",
    "        c_img = cv2.imread(c_img_fpath)\n",
    "        c_img = cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)\n",
    "        n_img = cv2.imread(n_img_fpath)\n",
    "        n_img = cv2.cvtColor(n_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        transform_input = {\n",
    "            'image': c_img,\n",
    "            'image2': n_img\n",
    "        }\n",
    "        transform_result = self.transform(**transform_input)\n",
    "        \n",
    "        c_img = transform_result['image']\n",
    "        n_img = transform_result['image2']\n",
    "        \n",
    "        def image2tensor(img):\n",
    "            img = img.transpose(2,0,1)\n",
    "            img = img / 255 * 2 - 1\n",
    "            tnsr = torch.from_numpy(img.astype(np.float32))\n",
    "            \n",
    "#         T = n_pose @ np.linalg.inv(c_pose)\n",
    "#         print('CPose\\n', c_pose)\n",
    "#         print('NPose\\n', n_pose)\n",
    "#         print('Transform\\n', T)\n",
    "#         print('CEuler\\n', Rotation.from_matrix(c_pose[:3,:3]).as_euler('zxy', degrees=True))\n",
    "#         print('NEuler\\n', Rotation.from_matrix(n_pose[:3,:3]).as_euler('zxy', degrees=True))\n",
    "\n",
    "        local_dtrans = np.linalg.inv(c_pose) @ n_pose[:, 3]\n",
    "#         print('Local trans', local_dtrans)\n",
    "        \n",
    "        quat_c = quat.from_rotation_matrix(c_pose[:3,:3])\n",
    "        quat_n = quat.from_rotation_matrix(n_pose[:3,:3])\n",
    "        quat_t = quat_c.inverse() * quat_n\n",
    "#         print('CQ\\n', quat_c)\n",
    "#         print('NQ\\n', quat_n)\n",
    "#         print('TQ\\n', quat_t)\n",
    "        \n",
    "        # Recovery process\n",
    "        n_pose_rec = np.eye(4)\n",
    "        n_pose_rec[:3, :3] = quat.as_rotation_matrix(quat_c * quat_t)\n",
    "        n_pose_rec[:, 3] = c_pose @ local_dtrans\n",
    "#         print('NPose (recovered)\\n', n_pose_rec)\n",
    "#         print('Diff\\n', np.abs(n_pose-n_pose_rec))\n",
    "        \n",
    "        c_tnsr = image2tensor(c_img)\n",
    "        n_tnsr = image2tensor(n_img)\n",
    "        \n",
    "        if self.debug:\n",
    "            return c_img, n_img, quat.as_float_array(quat_t), local_dtrans\n",
    "        else:\n",
    "            return c_tnsr, n_tnsr, quat.as_float_array(quat_t), local_dtrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = os.path.join('../', 'data/KITTI/dataset')\n",
    "transform = get_transform()\n",
    "dataset = KITTIDataset(DATASET_DIR, transform, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_img, n_img, quaternion, trns = dataset[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.imshow(c_img)\n",
    "print(c_img.shape)\n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "plt.imshow(n_img)\n",
    "print('Trans', trns)\n",
    "print('Quat', quaternion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosePrediction(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = torch.hub.load('pytorch/vision:v0.5.0', 'resnet18', pretrained=True)\n",
    "        self.backbone = nn.Sequential(*(list(self.backbone.children())[:-2]))\n",
    "        \n",
    "        self.predict = nn.Sequential(\n",
    "            *[\n",
    "                nn.Conv2d(1024, 1024, 3),\n",
    "                nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.Conv2d(1024, 1024, 3),\n",
    "                nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(1024, 120),\n",
    "                nn.Linear(120, 84)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.fc_quat = nn.Linear(84, 4)\n",
    "        self.fc_trns = nn.Linear(84, 3)\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        \n",
    "        feat_img1 = self.backbone(img1)\n",
    "        feat_img2 = self.backbone(img2)\n",
    "        \n",
    "        x = torch.cat((feat_img1, feat_img2), dim=1)\n",
    "        \n",
    "        x = self.predict(x)\n",
    "        \n",
    "#         x = x.view([-1, 1024])\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        pred_quat = self.fc_quat(x)        \n",
    "        pred_trns = self.fc_trns(x)\n",
    "        \n",
    "        return pred_quat, pred_trns\n",
    "\n",
    "#     def extra_repr(self):\n",
    "#         return f'(backbone): {self.backbone}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PosePrediction()\n",
    "\n",
    "sample_input1 = torch.rand(\n",
    "    (2, 3, 224, 224)\n",
    ") \n",
    "\n",
    "sample_input2 = torch.rand(\n",
    "    (2, 3, 224, 224)\n",
    ") \n",
    "\n",
    "pred_quat, pred_trns = model(sample_input1, sample_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
