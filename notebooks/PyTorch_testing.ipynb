{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from shared import metrics\n",
    "from shared import common\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import quaternion as quat\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from shared.transforms import ResizeKeepingRatio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "import neptune\n",
    "neptune.init('kail4ek/visual-odometry')\n",
    "# https://ui.neptune.ai/kail4ek/visual-odometry/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_value = (127, 127, 127)\n",
    "\n",
    "def get_train_transform(target_wh=(1024, 320)):\n",
    "    return albu.Compose([\n",
    "#         albu.RandomRotate90(),\n",
    "#         albu.Flip(),\n",
    "#         albu.Transpose(),\n",
    "        albu.OneOf([\n",
    "            albu.IAAAdditiveGaussianNoise(),\n",
    "            albu.GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        albu.OneOf([\n",
    "            albu.MotionBlur(p=0.2),\n",
    "            albu.MedianBlur(blur_limit=3, p=0.1),\n",
    "            albu.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit=0, p=0.4),\n",
    "#         albu.OneOf([\n",
    "#             albu.OpticalDistortion(p=0.3),\n",
    "#             albu.GridDistortion(p=0.1),\n",
    "#             albu.IAAPiecewiseAffine(p=0.3),\n",
    "#         ], p=0.2),\n",
    "        albu.OneOf([\n",
    "            albu.CLAHE(clip_limit=2),\n",
    "            albu.IAASharpen(),\n",
    "            albu.IAAEmboss(),\n",
    "            albu.RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        albu.HueSaturationValue(p=0.3),\n",
    "        ResizeKeepingRatio(target_wh=target_wh, interpolation=cv2.INTER_CUBIC, always_apply=True),\n",
    "        albu.PadIfNeeded(min_height=target_wh[1],\n",
    "                         min_width=target_wh[0],\n",
    "                         border_mode=0,\n",
    "                         value=fill_value,\n",
    "                         always_apply=True)\n",
    "    ],\n",
    "    additional_targets={\"image2\" : \"image\"})\n",
    "\n",
    "def get_valid_transform(target_wh=(1024, 320)):\n",
    "    return albu.Compose([\n",
    "        ResizeKeepingRatio(target_wh=target_wh, interpolation=cv2.INTER_CUBIC, always_apply=True),\n",
    "        albu.PadIfNeeded(min_height=target_wh[1],\n",
    "                         min_width=target_wh[0],\n",
    "                         border_mode=0,\n",
    "                         value=fill_value,\n",
    "                         always_apply=True)\n",
    "    ],\n",
    "    additional_targets={\"image2\" : \"image\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, transform, sequence_id='00', debug=False):\n",
    "        self.debug = debug\n",
    "        \n",
    "        SEQUENCE_DIR = os.path.join(DATASET_DIR, 'sequences', sequence_id)\n",
    "        POSES_DIR = os.path.join(DATASET_DIR, 'poses')\n",
    "\n",
    "        POSE_PATH = os.path.join(POSES_DIR, f'{sequence_id}.txt')\n",
    "        TIMES_PATH = os.path.join(SEQUENCE_DIR, 'times.txt')\n",
    "\n",
    "        self.poses = self._load_poses(POSE_PATH)\n",
    "        self.times = self._load_times(TIMES_PATH)\n",
    "        \n",
    "        self.IMAGES_DIR = os.path.join(SEQUENCE_DIR, 'image_2')\n",
    "        self.images = [fname for fname in os.listdir(self.IMAGES_DIR) if fname.endswith('.png')]\n",
    "        \n",
    "        print(f'Sequence {sequence_id} length: {len(self.poses)}')\n",
    "        \n",
    "        # Sanity check!\n",
    "        for i in range(len(self.poses)):\n",
    "            fname = self._get_image_fname(i)\n",
    "            if fname not in self.images:\n",
    "                Exception(f'File with name {fname} not exists in {IMAGES_DIR}')\n",
    "        # After this check we can use idx to generate fpaths\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def _get_image_fname(self, idx):\n",
    "        return f'{idx:06}.png'\n",
    "        \n",
    "    def _load_times(self, fpath):\n",
    "        times_data = np.fromfile(fpath, sep='\\n')\n",
    "        return times_data\n",
    "    \n",
    "    def _load_poses(self, fpath):\n",
    "        poses_data = np.fromfile(fpath, sep=' ')\n",
    "        poses_data = poses_data.reshape((-1, 3, 4))\n",
    "        # Convert to 4x4 matrices\n",
    "        last_row = np.array([[[0,0,0,1]]])\n",
    "        last_rows = np.repeat(last_row, axis=0, repeats=poses_data.shape[0])\n",
    "        poses_data = np.hstack((poses_data, last_rows))\n",
    "        return poses_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poses)-1\n",
    "\n",
    "    def _get_transform(self, idx):\n",
    "        c_idx = idx\n",
    "        n_idx = idx+1\n",
    "        \n",
    "        c_pose = self.poses[c_idx]\n",
    "        n_pose = self.poses[n_idx]\n",
    "\n",
    "        local_dtrans = np.linalg.inv(c_pose) @ n_pose[:, 3]\n",
    "\n",
    "        quat_c = quat.from_rotation_matrix(c_pose[:3,:3])\n",
    "        quat_n = quat.from_rotation_matrix(n_pose[:3,:3])\n",
    "        quat_t = quat_c.inverse() * quat_n\n",
    "\n",
    "        gt_quat_t_ar = quat.as_float_array(quat_t).astype(np.float32)\n",
    "        gt_trans = local_dtrans[:3].astype(np.float32)\n",
    "\n",
    "        return gt_quat_t_ar, gt_trans\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        c_idx = idx\n",
    "        n_idx = idx+1\n",
    "        \n",
    "        c_pose = self.poses[c_idx]\n",
    "        n_pose = self.poses[n_idx]\n",
    "\n",
    "        c_img_fpath = os.path.join(\n",
    "            self.IMAGES_DIR, \n",
    "            self._get_image_fname(c_idx)\n",
    "        )\n",
    "        n_img_fpath = os.path.join(\n",
    "            self.IMAGES_DIR, \n",
    "            self._get_image_fname(n_idx)\n",
    "        )\n",
    "        \n",
    "        c_img = cv2.imread(c_img_fpath)\n",
    "        c_img = cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)\n",
    "        n_img = cv2.imread(n_img_fpath)\n",
    "        n_img = cv2.cvtColor(n_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        transform_input = {\n",
    "            'image': c_img,\n",
    "            'image2': n_img\n",
    "        }\n",
    "        transform_result = self.transform(**transform_input)\n",
    "        \n",
    "        c_img = transform_result['image']\n",
    "        n_img = transform_result['image2']\n",
    "        \n",
    "        def image2tensor(img):\n",
    "            img = img.transpose(2,0,1)\n",
    "            img = img / 255 * 2 - 1\n",
    "            tnsr = torch.from_numpy(img.astype(np.float32))\n",
    "            return tnsr\n",
    "            \n",
    "        gt_quat_t_ar, gt_trans = self._get_transform(idx)\n",
    "         \n",
    "        c_tnsr = image2tensor(c_img)\n",
    "        n_tnsr = image2tensor(n_img)\n",
    "            \n",
    "        gt_quat_t_ar = torch.from_numpy(gt_quat_t_ar)\n",
    "        gt_trans = torch.from_numpy(gt_trans)\n",
    "            \n",
    "        if self.debug:\n",
    "            return c_img, n_img, gt_quat_t_ar, gt_trans\n",
    "        else:\n",
    "            return c_tnsr, n_tnsr, gt_quat_t_ar, gt_trans\n",
    "    \n",
    "    \n",
    "def predict_2_next_pose(c_pose, pred_quat, pred_trans):\n",
    "    n_pose = np.eye(4)\n",
    "    quat_t = quat.from_float_array(pred_quat)\n",
    "    quat_c = quat.from_rotation_matrix(c_pose[:3,:3])\n",
    "    n_pose[:3, :3] = quat.as_rotation_matrix(quat_c * quat_t)\n",
    "    n_pose[:, 3] = c_pose @ np.array([*pred_trans, 1])\n",
    "\n",
    "    return n_pose\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_wh = (1024, 320)\n",
    "\n",
    "DATASET_DIR = os.path.join('../', 'data/KITTI/dataset')\n",
    "train_transform = get_train_transform()\n",
    "valid_transform = get_valid_transform()\n",
    "dataset = KITTIDataset(DATASET_DIR, train_transform, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_img, n_img, quaternion, trns = dataset[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.imshow(c_img)\n",
    "print(c_img.shape)\n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "plt.imshow(n_img)\n",
    "print('Trans', trns)\n",
    "print('Quat', quaternion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosePrediction(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = torch.hub.load('pytorch/vision:v0.5.0', 'resnet18', pretrained=True)\n",
    "        self.backbone = nn.Sequential(*(list(self.backbone.children())[:-2]))\n",
    "        \n",
    "        self.predict = nn.Sequential(\n",
    "            *[\n",
    "                nn.Conv2d(1024, 1024, 3),\n",
    "                nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.Conv2d(1024, 1024, 3),\n",
    "                nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(1024, 120),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Dropout(p=0.5)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.fc_quat = nn.Linear(84, 4)\n",
    "        self.fc_trns = nn.Linear(84, 3)\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        \n",
    "        feat_img1 = self.backbone(img1)\n",
    "        feat_img2 = self.backbone(img2)\n",
    "        \n",
    "        x = torch.cat((feat_img1, feat_img2), dim=1)\n",
    "        \n",
    "        x = self.predict(x)\n",
    "        \n",
    "#         x = x.view([-1, 1024])\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        pred_quat = self.fc_quat(x)        \n",
    "        pred_quat = F.normalize(pred_quat)\n",
    "        \n",
    "        pred_trns = self.fc_trns(x)\n",
    "        \n",
    "        return pred_quat, pred_trns\n",
    "\n",
    "#     def extra_repr(self):\n",
    "#         return f'(backbone): {self.backbone}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PosePrediction()\n",
    "\n",
    "sample_input1 = torch.rand(\n",
    "    (2, 3, 224, 224)\n",
    ") \n",
    "\n",
    "sample_input2 = torch.rand(\n",
    "    (2, 3, 224, 224)\n",
    ") \n",
    "\n",
    "pred_quat, pred_trns = model(sample_input1, sample_input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trajectory collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KITTIDataset(DATASET_DIR, train_transform, sequence_id=\"00\", debug=False)\n",
    "\n",
    "gt_poses = [np.eye(4)]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    gt_quat, gt_trns = dataset._get_transform(i)\n",
    "    \n",
    "    n_pose = predict_2_next_pose(gt_poses[-1], gt_quat, gt_trns)\n",
    "    gt_poses.append(n_pose)\n",
    "    \n",
    "metrics_ate = metrics.compute_ATE(gt_poses, dataset.poses)\n",
    "print(f'Metrics ATE: {metrics_ate}')\n",
    "    \n",
    "plt.figure(figsize=[9,9])\n",
    "common.plot_trajectory(gt_poses)\n",
    "    \n",
    "plt.figure(figsize=[9,9])\n",
    "common.plot_trajectory(dataset.poses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in https://www.cs.ox.ac.uk/files/9026/DeepVO.pdf\n",
    "train_sequences = ['00', '02', '08', '09']\n",
    "valid_sequences = ['01', '03', '04', '05', '06', '07', '10']\n",
    "\n",
    "train_datasets = []\n",
    "valid_datasets = []\n",
    "\n",
    "train_transform = get_train_transform()\n",
    "valid_transform = get_valid_transform()\n",
    "\n",
    "for seq in train_sequences:\n",
    "    dataset = KITTIDataset(DATASET_DIR, train_transform, sequence_id=seq, debug=False)\n",
    "    train_datasets.append(dataset)\n",
    "    \n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "\n",
    "for seq in valid_sequences:\n",
    "    dataset = KITTIDataset(DATASET_DIR, valid_transform, sequence_id=seq, debug=False)\n",
    "    valid_datasets.append(dataset)\n",
    "    \n",
    "valid_dataset = ConcatDataset(valid_datasets)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "\n",
    "params = {\n",
    "    'input_wh': model_input_wh,\n",
    "    'initial_lr': 1e-3,\n",
    "    'epochs': 500\n",
    "}\n",
    "\n",
    "criterion_quat = nn.MSELoss()\n",
    "criterion_trns = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['initial_lr'])\n",
    "\n",
    "scheduler_cfg = {\n",
    "    'mode': 'min',\n",
    "    'factor': 0.5,\n",
    "    'patience': 30,\n",
    "    'verbose': False,\n",
    "    'threshold': 0.0001,\n",
    "    'threshold_mode': 'rel',\n",
    "    'cooldown': 0,\n",
    "    'min_lr': 1e-6,\n",
    "    'eps': 1e-08\n",
    "}\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, **scheduler_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_metrics(dataset, model):\n",
    "    poses = [np.eye(4)]\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=8)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:          \n",
    "            c_img, n_img, gt_quat, gt_trns = move_2_device(data)\n",
    "\n",
    "            pred_quat, pred_trns = model(c_img, n_img)\n",
    "            pred_quat = pred_quat.cpu()\n",
    "            pred_trns = pred_trns.cpu()\n",
    "\n",
    "            for i in range(pred_quat.shape[0]):\n",
    "                n_pose = predict_2_next_pose(poses[-1], pred_quat[i], pred_trns[i])\n",
    "                poses.append(n_pose)\n",
    "\n",
    "    metrics_ate = metrics.compute_ATE(poses, dataset.poses)\n",
    "    rpe_trns, rpe_rot = metrics.compute_RPE(poses, dataset.poses)\n",
    "    \n",
    "    return [metrics_ate, rpe_trns, rpe_rot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_device = torch.device('cuda')\n",
    "model.to(target_device)\n",
    "\n",
    "def move_2_device(data):\n",
    "    return [el.to(target_device) for el in data]\n",
    "\n",
    "modes = ['train', 'valid']\n",
    "\n",
    "# Test metrics calculation before start\n",
    "dataset = KITTIDataset(DATASET_DIR, valid_transform, sequence_id=\"01\", debug=False)\n",
    "seq_metrics = get_sequence_metrics(dataset, model)\n",
    "\n",
    "assert len(seq_metrics) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tags = []\n",
    "    \n",
    "neptune.create_experiment(\n",
    "#     name=utils.get_neptune_name(config),\n",
    "    upload_stdout=False,\n",
    "#     upload_source_files=sources_to_upload,\n",
    "    params=params,\n",
    "    tags=tags\n",
    ")\n",
    "\n",
    "best_val_loss = 10e10\n",
    "\n",
    "date = datetime.today().strftime('%Y%m%d-%H%M%S')\n",
    "chk_dir = f'../data/chks/{date}'\n",
    "os.makedirs(chk_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(params['epochs']):  # loop over the dataset multiple times\n",
    "    for mode in modes:\n",
    "        if mode == 'train':\n",
    "            dataloader = train_loader\n",
    "            model.train()\n",
    "        else:\n",
    "            dataloader = valid_loader\n",
    "            model.eval()\n",
    "\n",
    "        loss_vals = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            c_img, n_img, gt_quat, gt_trns = move_2_device(data)\n",
    "            optimizer.zero_grad()\n",
    "            pred_quat, pred_trns = model(c_img, n_img)\n",
    "\n",
    "            loss_quat = criterion_quat(pred_quat, gt_quat)\n",
    "            loss_trns = criterion_trns(pred_trns, gt_trns)\n",
    "            loss = loss_quat + loss_trns\n",
    "\n",
    "            if mode == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            loss_vals.append(loss.item())\n",
    "\n",
    "        epoch_loss = np.mean(loss_vals)\n",
    "        print(f'{mode} loss: {epoch_loss}')\n",
    "        neptune.send_metric(f'{mode}_loss', epoch_loss)\n",
    "       \n",
    "        if mode == 'valid':\n",
    "            scheduler.step(epoch_loss)\n",
    "            \n",
    "            if epoch_loss <= best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                save_dict = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     '_model_config': model_config,\n",
    "#                     '_train_config': train_config\n",
    "                }\n",
    "                chkpnt_fpath = os.path.join(chk_dir, f'val_loss_{epoch}.pthck')\n",
    "                \n",
    "                torch.save(save_dict, chkpnt_fpath)\n",
    "            \n",
    "            print('Perform sequence metrics collection')\n",
    "            val_metrics = []\n",
    "            for seq in valid_sequences:\n",
    "                dataset = KITTIDataset(DATASET_DIR, valid_transform, sequence_id=seq, debug=False)\n",
    "                seq_metrics = get_sequence_metrics(dataset, model)\n",
    "                val_metrics.append(seq_metrics)\n",
    "\n",
    "            val_mean_metrics = np.mean(val_metrics, axis=0)\n",
    "\n",
    "            neptune.send_metric('ATE', val_mean_metrics[0])\n",
    "            neptune.send_metric('RPE_T', val_mean_metrics[1])\n",
    "            neptune.send_metric('RPE_R', val_mean_metrics[2])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "neptune.stop()    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
