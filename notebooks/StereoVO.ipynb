{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main KITTI readme (old): https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT  \n",
    "- About Intrinsics: http://ksimek.github.io/2013/08/13/intrinsic/\n",
    "- Car scheme: http://www.cvlibs.net/publications/Geiger2013IJRR.pdf\n",
    "- Image with reprojection: https://yadi.sk/i/JAIIsbP5dHAELg\n",
    "- http://www.cs.toronto.edu/~urtasun/courses/CSC2541/03_odometry.pdf\n",
    "- https://dsp.stackexchange.com/questions/2736/step-by-step-camera-pose-estimation-for-visual-tracking-and-planar-markers\n",
    "- https://www.ifi.uzh.ch/dam/jcr:5759a719-55db-4930-8051-4cc534f812b1/VO_Part_I_Scaramuzza.pdf\n",
    "- https://answers.opencv.org/question/182049/pythonstereo-disparity-quality-problems/\n",
    "- https://pythonawesome.com/a-toy-implementation-of-a-visual-odometry-vo-pipeline-in-python-2/\n",
    "\n",
    "## Realizations:\n",
    "- https://github.com/cgarg92/Stereo-visual-odometry\n",
    "- https://github.com/ZhenghaoFei/visual_odom\n",
    "- https://avisingh599.github.io/vision/visual-odometry-full/\n",
    "- https://github.com/uoip/monoVO-python\n",
    "- \n",
    "\n",
    "## Helpful\n",
    "- https://www.youtube.com/watch?v=fEaHN8FsW_E\n",
    "\n",
    "## Paper\n",
    "- https://www.mdpi.com/1424-8220/18/9/2837/pdf\n",
    "- http://www.cvlibs.net/projects/autonomous_vision_survey/literature/Cvisic2015ECMR.pdf\n",
    "- http://cvg.dsi.unifi.it/pdfs/mva2016.pdf\n",
    "- https://www.scitepress.org/Papers/2018/66236/66236.pdf\n",
    "- https://pdfs.semanticscholar.org/879f/70a13aa7e461ec2425093f47475ac601a550.pdf\n",
    "- http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Stereo_DSO_Large-Scale_ICCV_2017_paper.pdf\n",
    "\n",
    "## Links\n",
    "### Corner Detection\n",
    "https://medium.com/data-breach/introduction-to-harris-corner-detector-32a88850b3f6\n",
    "https://aishack.in/tutorials/windows-harris-corner-detector/\n",
    "http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf\n",
    "https://aishack.in/tutorials/subpixel-corners-increasing-accuracy/\n",
    "\n",
    "https://aishack.in/tutorials/shitomasi-corner-detector/\n",
    "\n",
    "### SIFT\n",
    "https://towardsdatascience.com/sift-scale-invariant-feature-transform-c7233dc60f37\n",
    "https://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/\n",
    "(Source) https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n",
    "https://gist.github.com/lxc-xx/7088609\n",
    "\n",
    "### Features\n",
    "https://dspace.spbu.ru/bitstream/11701/3991/1/st010290.pdf\n",
    "\n",
    "### BRIEF\n",
    "https://medium.com/data-breach/introduction-to-brief-binary-robust-independent-elementary-features-436f4a31a0e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from shared.data import KITTIData,  VisualOdometry, draw_matches, draw_keypoints\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "print(mpl.style.available)\n",
    "mpl.style.use('grayscale')\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# https://github.com/matplotlib/ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = os.path.join('../', 'data/KITTI/dataset')\n",
    "dataset = KITTIData(DATASET_DIR, sequence_id=\"00\")\n",
    "\n",
    "# 00 / ~543 --- Waiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_3d_features(c_pnts3d, n_pnts3d, C_left):\n",
    "    P_left = np.hstack((C_left, np.transpose([[0, 0, 0]])))\n",
    "\n",
    "    c_pnts_2d = vo.reproject_3d_to_2d(c_pnts3d, P_left)\n",
    "    n_pnts_2d = vo.reproject_3d_to_2d(n_pnts3d, P_left)\n",
    "\n",
    "    c_img_canvas = c_l_img.copy()\n",
    "    n_img_canvas = n_l_img.copy()\n",
    "    # Check after clique and projection\n",
    "    draw_keypoints(c_img_canvas, n_img_canvas, c_pnts_2d, n_pnts_2d)\n",
    "    # # Rendering valid features\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "    ax1.imshow(c_img_canvas)\n",
    "    ax2.imshow(n_img_canvas)\n",
    "    \n",
    "def show_flow(c_pnts2d, n_pnts2d, img):\n",
    "    img = img.copy()\n",
    "    \n",
    "    r = 3\n",
    "    thickness = 2\n",
    "    c = (0, 255, 0)        \n",
    "    for i in range(len(c_pnts2d)):\n",
    "        c_pnt = c_pnts2d[i]\n",
    "        n_pnt = n_pnts2d[i]\n",
    "        end1 = tuple(np.round(c_pnt).astype(int))\n",
    "        end2 = tuple(np.round(n_pnt).astype(int))\n",
    "        cv2.line(img, end1, end2, c, thickness)\n",
    "        cv2.circle(img, end1, r, c, thickness)\n",
    "        cv2.circle(img, end2, r, c, thickness)\n",
    "    \n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_frame_idx = 10\n",
    "n_frame_idx = c_frame_idx+1\n",
    "c_l_img, c_r_img = dataset.get_images(c_frame_idx)\n",
    "n_l_img, n_r_img = dataset.get_images(n_frame_idx)\n",
    "PL, PR = dataset.get_P_matrix()\n",
    "Q_left = dataset.get_left_Q_matrix()\n",
    "c_pose = dataset.get_poses()[c_frame_idx]\n",
    "n_pose = dataset.get_poses()[n_frame_idx]\n",
    "C_left, _ = dataset.get_ะก_matrix()\n",
    "_, gt_trnsl = dataset._get_transform(c_frame_idx, n_frame_idx)\n",
    "\n",
    "vo = VisualOdometry()\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "# ax1.imshow(c_l_img)\n",
    "# ax2.imshow(n_l_img)\n",
    "print(PL)\n",
    "print(PR)\n",
    "print(Q_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_feats, cr_feats, nl_feats, nr_feats = vo.get_circular_features(c_l_img, c_r_img, n_l_img, n_r_img, lk_err=20, y_err=2)\n",
    "\n",
    "n_pnts_3d = vo.get_3d_points(nl_feats, nr_feats, PL, PR)\n",
    "c_pnts_2d = cl_feats.copy()\n",
    "\n",
    "ess_transform, mask = vo.get_relative_transform(cl_feats, nl_feats, C_left)\n",
    "ess_rvec = cv2.Rodrigues(ess_transform[:3,:3])[0][:,0]\n",
    "ess_tvec = ess_transform[:3,3].copy()\n",
    "\n",
    "solver_data = {\n",
    "#     'tvec': ess_tvec,\n",
    "#     'rvec': ess_rvec\n",
    "}\n",
    "\n",
    "print(f'Essential T:\\n{ess_transform}')\n",
    "transform = vo.get_transform(c_pnts_2d, None, None, n_pnts_3d, C_left, type_='PnPRansac', solver_data=solver_data)\n",
    "print(f'Stereo T:\\n{transform}')\n",
    "print(f'gt_t: {gt_trnsl}')\n",
    "print(f'pred_t: {transform[:3,3]}')\n",
    "print(f'c_pose: {c_pose[:3,3]}')\n",
    "print(f'n_pose: {n_pose[:3,3]}')\n",
    "scale = np.linalg.norm(n_pose[:3,3]-c_pose[:3,3])\n",
    "print(f'Scale: {scale}')\n",
    "      \n",
    "n_pred_pose = vo.get_next_pose(transform, c_pose)\n",
    "err = np.abs(n_pose-n_pred_pose)\n",
    "print('Errors:')\n",
    "print(err[:3,:3])\n",
    "print(err[:3,3])\n",
    "\n",
    "inl_idxs = solver_data['inliers'][:,0]\n",
    "cl_feats_rnsc = cl_feats[inl_idxs]\n",
    "cr_feats_rnsc = cr_feats[inl_idxs]\n",
    "nl_feats_rnsc = nl_feats[inl_idxs]\n",
    "nr_feats_rnsc = nr_feats[inl_idxs]\n",
    "\n",
    "fix_threshold = 35\n",
    "flows = np.linalg.norm(cl_feats_rnsc-nl_feats_rnsc, axis=1)\n",
    "print(flows[flows > fix_threshold].size/flows.size)\n",
    "\n",
    "\n",
    "c_img_canvas = c_l_img.copy()\n",
    "n_img_canvas = n_l_img.copy()\n",
    "draw_keypoints(c_img_canvas, n_img_canvas, cl_feats, nl_feats)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(c_img_canvas)\n",
    "ax2.imshow(n_img_canvas)\n",
    "\n",
    "# show_3d_features(c_pnts_3d, n_pnts_3d, C_left)\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(cl_feats_rnsc, nl_feats_rnsc, n_l_img)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(cl_feats, cr_feats, c_l_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_feats, cr_feats, nl_feats, nr_feats = vo.get_circular_features_akaze(c_l_img, c_r_img, n_l_img, n_r_img, y_err=1)\n",
    "\n",
    "n_pnts_3d = vo.get_3d_points(nl_feats, nr_feats, PL, PR)\n",
    "c_pnts_2d = cl_feats.copy()\n",
    "\n",
    "ess_transform, mask = vo.get_relative_transform(cl_feats, nl_feats, C_left)\n",
    "ess_rvec = cv2.Rodrigues(ess_transform[:3,:3])[0][:,0]\n",
    "ess_tvec = ess_transform[:3,3].copy()\n",
    "\n",
    "solver_data = {\n",
    "#     'tvec': ess_tvec,\n",
    "#     'rvec': ess_rvec\n",
    "}\n",
    "\n",
    "print(f'Essential T:\\n{ess_transform}')\n",
    "transform = vo.get_transform(c_pnts_2d, None, None, n_pnts_3d, C_left, type_='PnPRansac', solver_data=solver_data)\n",
    "print(f'Stereo T:\\n{transform}')\n",
    "print(f'gt_t: {gt_trnsl}')\n",
    "print(f'pred_t: {transform[:3,3]}')\n",
    "print(f'c_pose: {c_pose[:3,3]}')\n",
    "print(f'n_pose: {n_pose[:3,3]}')\n",
    "scale = np.linalg.norm(n_pose[:3,3]-c_pose[:3,3])\n",
    "print(f'Scale: {scale}')\n",
    "      \n",
    "n_pred_pose = vo.get_next_pose(transform, c_pose)\n",
    "err = np.abs(n_pose-n_pred_pose)\n",
    "print('Errors:')\n",
    "print(err[:3,:3])\n",
    "print(err[:3,3])\n",
    "\n",
    "inl_idxs = solver_data['inliers'][:,0]\n",
    "cl_feats_rnsc = cl_feats[inl_idxs]\n",
    "nl_feats_rnsc = nl_feats[inl_idxs]\n",
    "nr_feats_rnsc = nr_feats[inl_idxs]\n",
    "\n",
    "fix_threshold = 35\n",
    "flows = np.linalg.norm(cl_feats_rnsc-nl_feats_rnsc, axis=1)\n",
    "print(flows[flows > fix_threshold].size/flows.size)\n",
    "\n",
    "\n",
    "c_img_canvas = c_l_img.copy()\n",
    "n_img_canvas = n_l_img.copy()\n",
    "draw_keypoints(c_img_canvas, n_img_canvas, cl_feats, nl_feats)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(c_img_canvas)\n",
    "ax2.imshow(n_img_canvas)\n",
    "\n",
    "# show_3d_features(c_pnts_3d, n_pnts_3d, C_left)\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(cl_feats_rnsc, nl_feats_rnsc, n_l_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_img = c_l_img.copy()\n",
    "r_img = c_r_img.copy()\n",
    "\n",
    "\n",
    "# ftrs = cv2.ORB_create()\n",
    "# ftrs = cv2.AKAZE_create()\n",
    "ftrs = cv2.BRISK_create()\n",
    "\n",
    "matcher= cv2.BFMatcher()\n",
    "\n",
    "# FLANN_INDEX_KDTREE = 1\n",
    "# index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "# search_params = dict(checks=50)\n",
    "# matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "if len(l_img.shape) == 3 and l_img.shape[2] > 1:\n",
    "    l_img = cv2.cvtColor(l_img, cv2.COLOR_RGB2GRAY)\n",
    "    r_img = cv2.cvtColor(r_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "l_kp, l_des = ftrs.detectAndCompute(l_img, None)\n",
    "r_kp, r_des = ftrs.detectAndCompute(r_img, None)\n",
    "\n",
    "l_des = np.float32(l_des)\n",
    "r_des = np.float32(r_des)\n",
    "\n",
    "\n",
    "# matches = matcher.knnMatch(l_des, r_des, k=2)\n",
    "matches = matcher.match(l_des, r_des)\n",
    "\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "l_pts = []\n",
    "r_pts = []\n",
    "\n",
    "good_matches = []\n",
    "for m in matches:\n",
    "    pt1 = l_kp[m.queryIdx].pt\n",
    "    pt2 = r_kp[m.trainIdx].pt\n",
    "    \n",
    "    y_err = abs(pt1[1]-pt2[1])\n",
    "    x_err = abs(pt1[0]-pt2[0])\n",
    "    \n",
    "    if y_err < 1 and x_err < 200:\n",
    "        good_matches.append(m)\n",
    "        l_pts.append(pt1)\n",
    "        r_pts.append(pt2)\n",
    "    \n",
    "\n",
    "# l_pts = np.array([f.pt for f in l_kp], dtype=int)\n",
    "# r_pts = np.array([f.pt for f in r_kp], dtype=int)\n",
    "# err = np.abs(l_pts-r_pts)\n",
    "# correct_idxs = (err[:,1] < 2)\n",
    "\n",
    "matches_img = draw_matches(l_img, l_kp, r_img, r_kp, good_matches, color=0, radius=5)\n",
    "plt.figure(figsize=[14,8])\n",
    "plt.imshow(matches_img)\n",
    "\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(l_pts, r_pts, l_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_img = c_l_img.copy()\n",
    "n_img = n_l_img.copy()\n",
    "\n",
    "\n",
    "# ftrs = cv2.AKAZE_create()\n",
    "ftrs = cv2.BRISK_create()\n",
    "# matcher= cv2.BFMatcher()\n",
    "matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMING)\n",
    "\n",
    "c_kp, c_des = ftrs.detectAndCompute(c_img, None)\n",
    "n_kp, n_des = ftrs.detectAndCompute(n_img, None)\n",
    "\n",
    "# c_des = np.float32(c_des)\n",
    "# n_des = np.float32(n_des)\n",
    "\n",
    "matches = matcher.knnMatch(c_des, n_des, 2)\n",
    "# matches = matcher.match(c_des, n_des)\n",
    "# matches = sorted(matches, key = lambda x:x.distance)\n",
    "c_pts = []\n",
    "n_pts = []\n",
    "\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.8 * n.distance:\n",
    "        pt1 = c_kp[m.queryIdx].pt\n",
    "        pt2 = n_kp[m.trainIdx].pt\n",
    "\n",
    "        y_err = abs(pt1[1]-pt2[1])\n",
    "        x_err = abs(pt1[0]-pt2[0])\n",
    "        err = np.array([x_err, y_err])\n",
    "\n",
    "        if np.linalg.norm(err) < 100:\n",
    "            good_matches.append(m)\n",
    "            c_pts.append(pt1)\n",
    "            n_pts.append(pt2)\n",
    "    \n",
    "\n",
    "# l_pts = np.array([f.pt for f in l_kp], dtype=int)\n",
    "# r_pts = np.array([f.pt for f in r_kp], dtype=int)\n",
    "# err = np.abs(l_pts-r_pts)\n",
    "# correct_idxs = (err[:,1] < 2)\n",
    "\n",
    "matches_img = draw_matches(c_img, c_kp, n_img, n_kp, good_matches, color=0, radius=5)\n",
    "plt.figure(figsize=[14,8])\n",
    "plt.imshow(matches_img)\n",
    "\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(c_pts, n_pts, c_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_depth_frame = vo.process_depth(c_l_img, c_r_img, Q_left)\n",
    "n_depth_frame = vo.process_depth(n_l_img, n_r_img, Q_left)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(c_depth_frame[:,:,2])\n",
    "ax2.imshow(n_depth_frame[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get 3D points \n",
    "c_pnts_3d, c_ft_idxs = vo.reproject_2d_to_3d_points(c_feats, c_depth_frame)\n",
    "n_pnts_3d, n_ft_idxs = vo.reproject_2d_to_3d_points(n_feats, n_depth_frame)\n",
    "\n",
    "ft_idxs = c_ft_idxs & n_ft_idxs\n",
    "\n",
    "c_pnts_3d = c_pnts_3d[ft_idxs]\n",
    "n_pnts_3d = n_pnts_3d[ft_idxs]\n",
    "c_feats = c_feats[ft_idxs]\n",
    "n_feats = n_feats[ft_idxs]\n",
    "\n",
    "# Find essential to get first estimation\n",
    "T, mask = vo.get_relative_transform(n_feats, c_feats, C_left)\n",
    "\n",
    "# c_feats_rnsc_ess = c_feats[mask]\n",
    "# n_feats_rnsc_ess = n_feats[mask]\n",
    "# c_pnts_3d_ess = c_pnts_3d[mask]\n",
    "# n_pnts_3d_ess = n_pnts_3d[mask]\n",
    "\n",
    "solver_data = {\n",
    "#     'tvec': t\n",
    "}\n",
    "transform = vo.get_transform(c_feats, n_feats, c_pnts_3d, n_pnts_3d, C_left, type_='PnPRansac', solver_data=solver_data)\n",
    "print(f'gt_t: {gt_trnsl}')\n",
    "print(f'pred_t: {transform[:3,3]}')\n",
    "n_pred_pose = vo.get_next_pose(transform, c_pose)\n",
    "err = np.abs(n_pose-n_pred_pose)\n",
    "print(err[:3,:3])\n",
    "print(err[:3,3])\n",
    "\n",
    "inl_idxs = solver_data['inliers'][:,0]\n",
    "c_feats_rnsc = c_feats[inl_idxs]\n",
    "n_feats_rnsc = n_feats[inl_idxs]\n",
    "\n",
    "show_3d_features(c_pnts_3d, n_pnts_3d, C_left)\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(c_feats_rnsc, n_feats_rnsc, n_l_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
