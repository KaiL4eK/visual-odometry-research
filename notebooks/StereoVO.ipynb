{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main KITTI readme (old): https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT  \n",
    "- About Intrinsics: http://ksimek.github.io/2013/08/13/intrinsic/\n",
    "- Car scheme: http://www.cvlibs.net/publications/Geiger2013IJRR.pdf\n",
    "- Image with reprojection: https://yadi.sk/i/JAIIsbP5dHAELg\n",
    "- http://www.cs.toronto.edu/~urtasun/courses/CSC2541/03_odometry.pdf\n",
    "- https://dsp.stackexchange.com/questions/2736/step-by-step-camera-pose-estimation-for-visual-tracking-and-planar-markers\n",
    "- https://www.ifi.uzh.ch/dam/jcr:5759a719-55db-4930-8051-4cc534f812b1/VO_Part_I_Scaramuzza.pdf\n",
    "- https://answers.opencv.org/question/182049/pythonstereo-disparity-quality-problems/\n",
    "- https://pythonawesome.com/a-toy-implementation-of-a-visual-odometry-vo-pipeline-in-python-2/\n",
    "\n",
    "## Realizations:\n",
    "- https://github.com/cgarg92/Stereo-visual-odometry\n",
    "- https://github.com/ZhenghaoFei/visual_odom\n",
    "- https://avisingh599.github.io/vision/visual-odometry-full/\n",
    "- https://github.com/uoip/monoVO-python\n",
    "- \n",
    "\n",
    "## Helpful\n",
    "- https://www.youtube.com/watch?v=fEaHN8FsW_E\n",
    "\n",
    "## Paper\n",
    "- https://www.mdpi.com/1424-8220/18/9/2837/pdf\n",
    "- http://www.cvlibs.net/projects/autonomous_vision_survey/literature/Cvisic2015ECMR.pdf\n",
    "- http://cvg.dsi.unifi.it/pdfs/mva2016.pdf\n",
    "- https://www.scitepress.org/Papers/2018/66236/66236.pdf\n",
    "- https://pdfs.semanticscholar.org/879f/70a13aa7e461ec2425093f47475ac601a550.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from shared.data import KITTIData,  VisualOdometry, draw_matches, draw_keypoints\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "print(mpl.style.available)\n",
    "mpl.style.use('grayscale')\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# https://github.com/matplotlib/ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = os.path.join('../', 'data/KITTI/dataset')\n",
    "dataset = KITTIData(DATASET_DIR, sequence_id=\"00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_frame_idx = 0\n",
    "n_frame_idx = c_frame_idx+1\n",
    "c_l_img, c_r_img = dataset.get_images(c_frame_idx)\n",
    "n_l_img, n_r_img = dataset.get_images(n_frame_idx)\n",
    "PL, PR = dataset.get_P_matrix()\n",
    "Q_left = dataset.get_left_Q_matrix()\n",
    "c_pose = dataset.get_poses()[c_frame_idx]\n",
    "n_pose = dataset.get_poses()[n_frame_idx]\n",
    "C_left, _ = dataset.get_ะก_matrix()\n",
    "_, gt_trnsl = dataset._get_transform(c_frame_idx, n_frame_idx)\n",
    "\n",
    "vo = VisualOdometry()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(c_l_img)\n",
    "ax2.imshow(n_l_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_feats, n_feats = vo.get_features(c_l_img, n_l_img)\n",
    "\n",
    "c_img_canvas = c_l_img.copy()\n",
    "n_img_canvas = n_l_img.copy()\n",
    "draw_keypoints(c_img_canvas, n_img_canvas, c_feats, n_feats)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(c_img_canvas)\n",
    "ax2.imshow(n_img_canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_feats, cr_feats = vo.get_disparity_features(c_l_img, c_r_img)\n",
    "\n",
    "cl_img_canvas = c_l_img.copy()\n",
    "cr_img_canvas = c_r_img.copy()\n",
    "draw_keypoints(cl_img_canvas, cr_img_canvas, cl_feats, cr_feats)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(cl_img_canvas)\n",
    "ax2.imshow(cr_img_canvas)\n",
    "\n",
    "c_pnts_3d = vo.get_3d_points(cl_feats, cr_feats, PL, PR)\n",
    "print(c_pnts_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_feats, cr_feats, nl_feats, nr_feats = vo.get_circular_features(c_l_img, c_r_img, n_l_img, n_r_img)\n",
    "\n",
    "c_img_canvas = c_l_img.copy()\n",
    "n_img_canvas = n_l_img.copy()\n",
    "draw_keypoints(c_img_canvas, n_img_canvas, cl_feats, nl_feats)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(c_img_canvas)\n",
    "ax2.imshow(n_img_canvas)\n",
    "\n",
    "\n",
    "# c_feats = np.vstack((cl_feats, cr_feats))\n",
    "# n_feats = np.vstack((nl_feats, nr_feats))\n",
    "# flows = np.linalg.norm(cl_feats-nl_feats, axis=1)\n",
    "# print(flows[flows > 20].size/flows.size)\n",
    "# print(flows)\n",
    "# idxs = flows > 35\n",
    "\n",
    "# cl_feats = cl_feats[idxs]\n",
    "# nl_feats = nl_feats[idxs]\n",
    "# nr_feats = nr_feats[idxs]\n",
    "\n",
    "n_pnts_3d = vo.get_3d_points(nl_feats, nr_feats, PL, PR)\n",
    "c_pnts_2d = cl_feats.copy()\n",
    "\n",
    "\n",
    "solver_data = {}\n",
    "transform = vo.get_transform(c_pnts_2d, None, None, n_pnts_3d, C_left, type_='PnPRansac', solver_data=solver_data)\n",
    "print(f'gt_t: {gt_trnsl}')\n",
    "print(f'pred_t: {transform[:3,3]}')\n",
    "n_pred_pose = vo.get_next_pose(transform, c_pose)\n",
    "err = np.abs(n_pose-n_pred_pose)\n",
    "print(err[:3,:3])\n",
    "print(err[:3,3])\n",
    "\n",
    "inl_idxs = solver_data['inliers'][:,0]\n",
    "cl_feats_rnsc = cl_feats[inl_idxs]\n",
    "cr_feats_rnsc = cr_feats[inl_idxs]\n",
    "nl_feats_rnsc = nl_feats[inl_idxs]\n",
    "nr_feats_rnsc = nr_feats[inl_idxs]\n",
    "\n",
    "fix_threshold = 35\n",
    "flows = np.linalg.norm(cl_feats_rnsc-nl_feats_rnsc, axis=1)\n",
    "print(flows[flows > fix_threshold].size/flows.size)\n",
    "\n",
    "show_3d_features(c_pnts_3d, n_pnts_3d, C_left)\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(cl_feats_rnsc, nl_feats_rnsc, n_l_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_img = c_l_img.copy()\n",
    "r_img = c_r_img.copy()\n",
    "\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "matcher= cv2.BFMatcher()\n",
    "\n",
    "# FLANN_INDEX_KDTREE = 1\n",
    "# index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "# search_params = dict(checks=50)\n",
    "# matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "if len(l_img.shape) == 3 and l_img.shape[2] > 1:\n",
    "    l_img = cv2.cvtColor(l_img, cv2.COLOR_RGB2GRAY)\n",
    "    r_img = cv2.cvtColor(r_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "l_kp, l_des = orb.detectAndCompute(l_img, None)\n",
    "r_kp, r_des = orb.detectAndCompute(r_img, None)\n",
    "\n",
    "l_des = np.float32(l_des)\n",
    "r_des = np.float32(r_des)\n",
    "\n",
    "\n",
    "matches = matcher.knnMatch(l_des, r_des, k=2)\n",
    "# matches = sorted(matches, key = lambda x:x.distance)\n",
    "l_pts = []\n",
    "r_pts = []\n",
    "\n",
    "good_matches = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "l_pts = np.array([f.pt for f in l_kp], dtype=int)\n",
    "r_pts = np.array([f.pt for f in r_kp], dtype=int)\n",
    "\n",
    "matches_img = draw_matches(l_img, l_kp, r_img, r_kp, good_matches, color=0, radius=5)\n",
    "plt.figure(figsize=[14,8])\n",
    "plt.imshow(matches_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_depth_frame = vo.process_depth(c_l_img, c_r_img, Q_left)\n",
    "n_depth_frame = vo.process_depth(n_l_img, n_r_img, Q_left)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "ax1.imshow(c_depth_frame[:,:,2])\n",
    "ax2.imshow(n_depth_frame[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_3d_features(c_pnts3d, n_pnts3d, C_left):\n",
    "    P_left = np.hstack((C_left, np.transpose([[0, 0, 0]])))\n",
    "\n",
    "    c_pnts_2d = vo.reproject_3d_to_2d(c_pnts3d, P_left)\n",
    "    n_pnts_2d = vo.reproject_3d_to_2d(n_pnts3d, P_left)\n",
    "\n",
    "    c_img_canvas = c_l_img.copy()\n",
    "    n_img_canvas = n_l_img.copy()\n",
    "    # Check after clique and projection\n",
    "    draw_keypoints(c_img_canvas, n_img_canvas, c_pnts_2d, n_pnts_2d)\n",
    "    # # Rendering valid features\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[14,10])\n",
    "    ax1.imshow(c_img_canvas)\n",
    "    ax2.imshow(n_img_canvas)\n",
    "    \n",
    "def show_flow(c_pnts2d, n_pnts2d, img):\n",
    "    img = img.copy()\n",
    "    \n",
    "    r = 3\n",
    "    thickness = 2\n",
    "    c = (0, 255, 0)        \n",
    "    for i in range(len(c_pnts2d)):\n",
    "        c_pnt = c_pnts2d[i]\n",
    "        n_pnt = n_pnts2d[i]\n",
    "        end1 = tuple(np.round(c_pnt).astype(int))\n",
    "        end2 = tuple(np.round(n_pnt).astype(int))\n",
    "        cv2.line(img, end1, end2, c, thickness)\n",
    "        cv2.circle(img, end1, r, c, thickness)\n",
    "        cv2.circle(img, end2, r, c, thickness)\n",
    "    \n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get 3D points \n",
    "c_pnts_3d, c_ft_idxs = vo.reproject_2d_to_3d_points(c_feats, c_depth_frame)\n",
    "n_pnts_3d, n_ft_idxs = vo.reproject_2d_to_3d_points(n_feats, n_depth_frame)\n",
    "\n",
    "ft_idxs = c_ft_idxs & n_ft_idxs\n",
    "\n",
    "c_pnts_3d = c_pnts_3d[ft_idxs]\n",
    "n_pnts_3d = n_pnts_3d[ft_idxs]\n",
    "c_feats = c_feats[ft_idxs]\n",
    "n_feats = n_feats[ft_idxs]\n",
    "\n",
    "# Find essential to get first estimation\n",
    "T, mask = vo.get_relative_transform(n_feats, c_feats, C_left)\n",
    "print(T)\n",
    "\n",
    "# rnsc_ess_idxs = mask[:,0]\n",
    "# c_feats_rnsc_ess = c_feats[rnsc_ess_idxs]\n",
    "# n_feats_rnsc_ess = n_feats[rnsc_ess_idxs]\n",
    "# c_pnts_3d_ess = c_pnts_3d[rnsc_ess_idxs]\n",
    "# n_pnts_3d_ess = n_pnts_3d[rnsc_ess_idxs]\n",
    "\n",
    "solver_data = {\n",
    "#     'tvec': t\n",
    "}\n",
    "transform = vo.get_transform(c_feats, n_feats, c_pnts_3d, n_pnts_3d, C_left, type_='PnPRansac', solver_data=solver_data)\n",
    "print(f'gt_t: {gt_trnsl}')\n",
    "print(f'pred_t: {transform[:3,3]}')\n",
    "n_pred_pose = vo.get_next_pose(transform, c_pose)\n",
    "err = np.abs(n_pose-n_pred_pose)\n",
    "print(err[:3,:3])\n",
    "print(err[:3,3])\n",
    "\n",
    "inl_idxs = solver_data['inliers'][:,0]\n",
    "c_feats_rnsc = c_feats[inl_idxs]\n",
    "n_feats_rnsc = n_feats[inl_idxs]\n",
    "\n",
    "show_3d_features(c_pnts_3d, n_pnts_3d, C_left)\n",
    "plt.figure(figsize=[14,5])\n",
    "show_flow(c_feats_rnsc, n_feats_rnsc, n_l_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
